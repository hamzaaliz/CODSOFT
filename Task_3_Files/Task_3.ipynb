{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# CodSoft Internship Task: 3\n## Iris Flower Classification",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Importing Necessary Libraries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Importing the necessary libraries.\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 52
    },
    {
      "cell_type": "markdown",
      "source": "### Loading the Dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Importing the dataset into a Pandas DataFrame.\ndataset = pd.read_csv('IRIS.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 53
    },
    {
      "cell_type": "markdown",
      "source": "### Cleaning and Preprocessing the Dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Converting the data-type of the target variable to categorical.\ndataset['species'] = dataset['species'].astype('category')\n\n# Performing log normalization on just the 'petal_length' column reduce it's variance.\ndataset['petal_length'] = np.log(dataset['petal_length'])\n\n# Performing feature scaling on the dataset.\nstandard_scaler = StandardScaler()\ndataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] = standard_scaler.fit_transform(dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 54
    },
    {
      "cell_type": "markdown",
      "source": "### Splitting the Datset into Training and Testing Sets",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = dataset.drop('species', axis=1)\ny = dataset['species']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 55
    },
    {
      "cell_type": "markdown",
      "source": "### Making Predictions Using KNN Classifier",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Initializing the KNN model with tuned hyperparamaters.\nknn = KNeighborsClassifier(metric='euclidean', n_neighbors=8, weights = 'distance')\n\n# Fit the model to the data.\nknn.fit(X_train, y_train)\n\n# Make predictions on the test set.\ny_pred = knn.predict(X_test)\n\n# Measure the accuracy of the model.\nprint('Accuracy:', accuracy_score(y_test, y_pred))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.9666666666666667\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 56
    }
  ]
}